https://sketchboard.me/FBkfcNuCdSkN#/


notes

great scoping and constraints
awesome at napkin math
good at thinking about checking if updated. 
compare on text. faster with hash and avoid query databse. 



SCOPE

crawler
takes a list of urls
generates a reverse index
output title and website snippet
you don't need to crawl its links, just the list

searcher
user inputs a term
gets back pages containing term in any order
imagine: search for an exact phrase

OUT OF SCOPE
Not personalised - no logins 
No analytics.
No ranking of results

USERS CONSTRAINTS
make 100 billion searches a month
search for some things more than other
want fresh results - we crawl once a week
they crawl rich websites - 500k each crawl
they want it to be fast and highly available 


THROUGHPUT
they have 1 billion links to search
1600 writes per second
40k searches per second

SERVICES

Scheduling
Starts job

CRAWLER

list links to crawl
list of crawled links
could use redis. can store 1b with no worries if you have memory
!!! discuss tradeoffs here between SQL and noSQL. 

How do we stop duplicates? 
!! How do we know when to crawl? Can we keep a time stamp?
!! Can we check if changed? 

get something from crawl queue
if crawled recenly, dump it
if not, make http request
parse http
if changed, save that to object db
add it to the crawled map with time last crawled

DOCUMENT
Stores title and snippet

SEARCH
Request to web server like nginx
Forwards to API

API
Sanitizes input
Does a lookup 
!!! How would you get fast results from many documents?
Use a reverse search index

DB
object database
relational

DATABASE
- Entire copy of page
- Sitemap


https://sketchboard.me/PBiQeK0kYtEK#/

SCALING

How do we stop duplicates at 1 billion websites? 

